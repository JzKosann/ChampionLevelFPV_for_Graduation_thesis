"""
train.py
:Author:  JinZ
:Create:  2024/10/6 10:59
:github profile:    https://github.com/JzKosann
:gitlab profile:    https://gitlab.com/JzKosann
Copyright (c) 2019, JinZ Group All Rights Reserved.

File content:
    traning code for unet in GateDetector function for graduation thesis
"""

from torch.utils.data import random_split
from torch.utils.tensorboard import SummaryWriter
from model.unet_model import UNet
from utils.dataset import MyDataLoader
from torch import optim
import torch.nn as nn
import torch
import os
import torch.nn.functional as F
from tqdm import tqdm
from colorama import Fore, Style
import emoji
from utils.anima import anima, train_complete_printf


def dice_loss(pred, target):
    """
    Dice Loss function to compute similarity between the predicted and ground truth segmentation.
    :param pred: predicted tensor from the model (N, C, H, W)
    :param target: ground truth tensor (N, C, H, W) with class labels
    :param smooth: a small constant to avoid division by zero
    :return: dice loss value
    """
    smooth = 1e-6  # å¹³æ»‘é¡¹ï¼ˆé¿å…é™¤ä»¥é›¶
    pred = F.softmax(pred, dim=1)  # è½¬æ¢æˆç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ
    pred_flat = pred.view(-1).float()
    target_flat = target.view(-1).float()
    intersection = (pred_flat * target_flat).sum()
    dice_score = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)
    return 1 - dice_score


def combined_loss(pred, target, alpha=0.5):
    """
    Combined loss function: weighted sum of CrossEntropyLoss and Dice Loss.
    :param pred: predicted tensor from the model (N, C, H, W)
    :param target: ground truth tensor (N, H, W) with class labels
    :param alpha: weight factor to balance CrossEntropy and Dice Loss
    :return: combined loss value
    """
    target_indexed = torch.argmax(target, dim=1)
    ce_loss = nn.CrossEntropyLoss()(pred, target_indexed)
    d_loss = dice_loss(pred, target)
    return alpha * ce_loss + (1 - alpha) * d_loss


def train_net(net, device, data_path, classesnum, checkpoint_path=None):
    """ ä¼ å…¥å‚æ•°
    :param net:ç½‘ç»œæ¨¡å‹
    :param device: GPUè¿˜æ˜¯CPU
    :param data_path: è®­ç»ƒé›†ç›®å½•
    :param classesnum: åˆ†ç±»æ•°é‡
    :return:

    """
    # ----------------------------#
    # å®šä¹‰
    epochs = 10000
    batch_size = 5
    lr = 0.003
    betas = (0.9, 0.999)
    weight_decay = 0
    accumulate_steps = 1
    _to_save_pth =False
    # åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†
    dataset = MyDataLoader(data_path, num_classes=classesnum)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=batch_size,
                                               shuffle=True)
    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,
                                             batch_size=batch_size,
                                             shuffle=False)
    # å®šä¹‰Adamç®—æ³• ä»¥åŠè‡ªå­¦ä¹ ç‡
    optimizer = optim.Adam(net.parameters(), lr=lr, betas=betas,
                           weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',
                                                     factor=0.9, patience=50, verbose=True)
    # best_val_lossç»Ÿè®¡ï¼Œåˆå§‹åŒ–ä¸ºæ­£æ— ç©· æœ€ä½³éªŒè¯é›†æŸå¤±
    best_val_loss = float('inf')
    best_train_loss = float('inf')
    # å»ºç«‹å¯è§†åŒ–
    visual_path = './runs'
    os.makedirs(visual_path, exist_ok=True)
    writer = SummaryWriter()

    start_epoch = 0
    # å¦‚æœæä¾›äº† checkpoint_pathï¼Œåˆ™åŠ è½½å·²æœ‰æ¨¡å‹
    if checkpoint_path and os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        net.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1  # ä»ä¸‹ä¸€ä¸ª epoch ç»§ç»­
        best_train_loss = checkpoint['train_loss']
        best_val_loss = checkpoint['val_loss']
        print(f"Loaded checkpoint from {checkpoint_path}, starting from epoch {start_epoch}")

    # è®­ç»ƒepochsæ¬¡
    for epoch in range(epochs):
        # è®­ç»ƒæ¨¡å¼
        net.train()
        train_loss = 0.0
        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader),
                            desc=f"Epoch {epoch + 1}/{epochs} ğŸš€")
        # æŒ‰ç…§batch_sizeå¼€å§‹è®­ç»ƒ
        for batch_idx, (image, label) in progress_bar:
            optimizer.zero_grad()
            # å°†æ•°æ®æ‹·è´åˆ°deviceä¸­
            image = image.to(device=device, dtype=torch.float32)
            label = label.to(device=device, dtype=torch.float32)  # ç¡®ä¿æ ‡ç­¾ä¸ºlongç±»å‹
            # ä½¿ç”¨ç½‘ç»œå‚æ•°ï¼Œè¾“å‡ºé¢„æµ‹ç»“æœ
            pred = net(image)
            # è®¡ç®—loss
            loss = combined_loss(pred, label, 0.5)
            loss.backward()
            if (batch_idx + 1) % accumulate_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
            train_loss += loss.item()
            progress_bar.set_postfix({
                f"{Fore.GREEN}live_Loss": f"{loss.item():.4f}{Style.RESET_ALL}",
                f"{Fore.YELLOW}Best Train Loss": f"{best_train_loss:.4f}{Style.RESET_ALL}",
                f"{Fore.RED}Best Val Loss": f"{best_val_loss:.4f}{Style.RESET_ALL}",
                f"{Fore.BLUE}LR": f"{optimizer.param_groups[0]['lr']:.10f}{Style.RESET_ALL}",
                "Step": f"{batch_idx + 1}/{len(train_loader)}",
                f"{Fore.LIGHTCYAN_EX}Status": f"{emoji.emojize('ğŸš€' if loss.item() < best_train_loss else 'âŒ›')}{Style.RESET_ALL}"
            })
            # ä¿å­˜losså€¼æœ€å°çš„ç½‘ç»œå‚æ•°
        avg_train_loss = train_loss / len(train_loader)
        if avg_train_loss < best_train_loss:
            best_train_loss = avg_train_loss
            _to_save_pth = True
        # è¯„ä¼°æ¨¡å¼ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        net.eval()
        val_loss = 0.0
        with torch.no_grad():
            for image, label in val_loader:
                image = image.to(device=device, dtype=torch.float32)
                label = label.to(device=device, dtype=torch.float32)

                pred = net(image)
                loss = combined_loss(pred, label, 0.5)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            _to_save_pth = True

        if _to_save_pth:
            torch.save({
                'epoch': epoch,
                'model_state_dict': net.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': best_train_loss,
                'val_loss': best_val_loss,
            }, 'best_model.pth')
            _to_save_pth = False

        # æ¯ä¸ª epoch ç»“æŸåè®°å½•åˆ° TensorBoard
        writer.add_scalar('Loss/train', avg_train_loss, epoch)
        writer.add_scalar('Loss/val', avg_val_loss, epoch)
        writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)
        scheduler.step(avg_val_loss)
    writer.close()
    ending_anima = True
    if ending_anima:
        anima()
        train_complete_printf()



if __name__ == "__main__":
    # é€‰æ‹©è®¾å¤‡ï¼Œæœ‰cudaç”¨cudaï¼Œæ²¡æœ‰å°±ç”¨cpu
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # åŠ è½½ç½‘ç»œï¼Œå›¾ç‰‡å•é€šé“n_channelï¼Œåˆ†ç±»ä¸ºn_classes
    net = UNet(n_channels=1, n_classes=5)
    # å°†ç½‘ç»œæ‹·è´åˆ°deivceä¸­
    net.to(device=device)
    # æŒ‡å®šè®­ç»ƒé›†åœ°å€ï¼Œå¼€å§‹è®­ç»ƒ
    data_path = "dataSet/"
    train_net(net, device, data_path, classesnum=5, checkpoint_path='./best_model.pth')
    # train_net(net, device, data_path, classesnum=5)

